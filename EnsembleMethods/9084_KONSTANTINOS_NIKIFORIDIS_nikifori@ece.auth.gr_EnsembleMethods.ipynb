{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. Make sure you fill in any place that says `# BEGIN CODE HERE #END CODE HERE`. After writing your code, you can run the cell by either pressing \"SHIFT\"+\"ENTER\" or by clicking on \"Run\" (denoted by a play symbol). Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). \n",
    "\n",
    " **What you need to remember:**\n",
    "\n",
    "- Run your cells using SHIFT+ENTER (or \"Run cell\")\n",
    "- Write code in the designated areas using Python 3 only\n",
    "- Do not modify the code outside of the designated areas\n",
    "- In some cases you will also need to explain the results. There will also be designated areas for that. \n",
    "\n",
    "Fill in your **NAME** and **AEM** below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Konstantinos Nikiforidis\"\n",
    "AEM = \"9084/ece_auth\"\n",
    "\n",
    "# Comment: H ergasia tha parei peripou na trexei olokliri 35-40 lepta! An krinw apo tous metrites poy exw valei.\n",
    "# Ola ta apotelesmata iparxoun kai se comments opote ama thelete gia oikonomia xronou mporeite na kanete ton kwdika\n",
    "# ctrl + / kai na trexete to notebook pio grigora. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1da26a63e48275bf64ed3608a92f75ac",
     "grade": false,
     "grade_id": "cell-28329c89a3d9ebb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 3 - Ensemble Methods #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e378be0e0c4ed85ebc6bcc53e256aa5",
     "grade": false,
     "grade_id": "cell-17ca53188deb1a2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Welcome to your third assignment. This exercise will test your understanding on Ensemble Methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fb9c467676bec1973d8e90bb815e23f",
     "grade": false,
     "grade_id": "cell-1a33a1efbf02238c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Always run this cell\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# USE THE FOLLOWING RANDOM STATE FOR YOUR CODE\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa2d11566658cb0f6d5ea19212a619de",
     "grade": false,
     "grade_id": "cell-7210caf6b2891007",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Download the Dataset ##\n",
    "Download the dataset using the following cell or from this [link](https://github.com/sakrifor/public/tree/master/machine_learning_course/EnsembleDataset) and put the files in the same folder as the .ipynb file. \n",
    "In this assignment you are going to work with a dataset originated from the [ImageCLEFmed: The Medical Task 2016](https://www.imageclef.org/2016/medical) and the **Compound figure detection** subtask. The goal of this subtask is to identify whether a figure is a compound figure (one image consists of more than one figure) or not. The train dataset consits of 4197 examples/figures and each figure has 4096 features which were extracted using a deep neural network. The *CLASS* column represents the class of each example where 1 is a compoung figure and 0 is not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c426c8680b4b28c9595139bec1d32f27",
     "grade": false,
     "grade_id": "cell-a413577b7685bfbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test_set_noclass.csv', <http.client.HTTPMessage at 0x1d6fb2905e0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url_train = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/train_set.csv'\n",
    "filename_train = 'train_set.csv'\n",
    "urllib.request.urlretrieve(url_train, filename_train)\n",
    "url_test = 'https://github.com/sakrifor/public/raw/master/machine_learning_course/EnsembleDataset/test_set_noclass.csv'\n",
    "filename_test = 'test_set_noclass.csv'\n",
    "urllib.request.urlretrieve(url_test, filename_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e073a9f617d5e2e192ef70d370f000b",
     "grade": false,
     "grade_id": "cell-cbadea205635117c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load the data\n",
    "train_set = pd.read_csv(\"train_set.csv\").sample(frac=1).reset_index(drop=True)\n",
    "train_set.head()\n",
    "X = train_set.drop(columns=['CLASS'])\n",
    "y = train_set['CLASS'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5c0ed51001ec5b7e6cdcef1303473e7",
     "grade": false,
     "grade_id": "cell-4f509bca5cb87e84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.0 Testing different ensemble methods ##\n",
    "In this part of the assignment you are asked to create and test different ensemble methods using the train_set.csv dataset. You should use **10-fold cross validation** for your tests and report the average f-measure and accuracy of your models.\n",
    "\n",
    "### !!! Use n_jobs=-1 where is posibble to use all the cores of a machine for running your tests ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d56eafac9ce59f34521ab931a24e9c8",
     "grade": false,
     "grade_id": "cell-db7468662add40fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.1 Voting ###\n",
    "Create a voting classifier which uses three estimators/classifiers. Test both soft and hard voting and choose the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bbf11fd8382f22bddabe61416516e7be",
     "grade": true,
     "grade_id": "cell-3a1719cdb031d112",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 1.1456552863121032 min\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cls1 = DecisionTreeClassifier(criterion=\"gini\", max_depth = 6, random_state=RANDOM_STATE) # Classifier #1 \n",
    "# DecisionTreeClassifier(max_depth=6, random_state=42)\n",
    "# F1-Score:0.7708696008456749 & Accuracy:0.7137697465621093\n",
    "cls2 = LogisticRegression(random_state=RANDOM_STATE) # Classifier #2 \n",
    "# LogisticRegression(random_state=42)\n",
    "# F1-Score:0.862477756053269 & Accuracy:0.8374582338902148\n",
    "cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=RANDOM_STATE) # Classifier #3\n",
    "vcls = VotingClassifier([(\"tree1\", cls1), (\"lg1\", cls2), (\"pnn\", cls3)], voting=\"soft\", n_jobs=-1) # Voting Classifier\n",
    "\n",
    "# cls1 = DecisionTreeClassifier(criterion=\"gini\", max_depth = 6, random_state=RANDOM_STATE) # Classifier #1 \n",
    "# cls2 = LogisticRegression(random_state=RANDOM_STATE) # Classifier #2 \n",
    "# cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=RANDOM_STATE) # Classifier #3\n",
    "# vcls = VotingClassifier([(\"tree1\", cls1), (\"lg1\", cls2), (\"pnn\", cls3)], voting=\"hard\", n_jobs=-1) # Voting Classifier\n",
    "\n",
    "# cv\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "all_scores = cross_validate(vcls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv, n_jobs=-1, verbose=1)\n",
    "\n",
    "\n",
    "avg_fmeasure = np.mean(all_scores[\"test_f1\"]) # The average f-measure\n",
    "avg_accuracy = np.mean(all_scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# hard voting: F1-Score:0.8695284385426904 & Accuracy:0.8448556654165247\n",
    "# soft voting: F1-Score:F1-Score:0.8735066055502333 & Accuracy:0.849850551199\n",
    "# Acc_soft > Acc_hard so we use soft_voting\n",
    "\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.1min finished\n",
    "exe_time = (time.time() - start_time)/60\n",
    "print(\"Execution time = {} min\".format(exe_time)) #1.1456552863121032 min\n",
    "\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f0e52e5eea2eb2cb23380c80ff846cf",
     "grade": false,
     "grade_id": "cell-0ef59e80595937ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "VotingClassifier(estimators=[('tree1',\n",
      "                              DecisionTreeClassifier(max_depth=6,\n",
      "                                                     random_state=42)),\n",
      "                             ('lg1', LogisticRegression(random_state=42)),\n",
      "                             ('pnn',\n",
      "                              MLPClassifier(hidden_layer_sizes=(15, 15),\n",
      "                                            random_state=42))],\n",
      "                 n_jobs=-1, voting='soft')\n",
      "F1-Score:0.8735066055502333 & Accuracy:0.849850551199\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(vcls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(avg_fmeasure,avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c7e6ab511ff3a546d2e2efb5feb892f",
     "grade": false,
     "grade_id": "cell-f6d620a3fd102626",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2 Stacking ###\n",
    "Create a stacking classifier which uses two estimators/classifiers. Try different classifiers for the combination of the initial classifiers. Report your results in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc7e22a168b668cbd10c524297950133",
     "grade": true,
     "grade_id": "cell-2ae5e38bd546681e",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 19.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 19.505288640658062 min\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "all_accs = []\n",
    "all_f1 = []\n",
    "\n",
    "cv_1 = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_2 = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# 1\n",
    "# cls1 = DecisionTreeClassifier(criterion=\"gini\", max_depth=11, random_state=RANDOM_STATE) # Classifier #1 \n",
    "# cls2 = LogisticRegression(random_state=RANDOM_STATE) # Classifier #2 \n",
    "# scls = StackingClassifier([(\"tree\", cls1), (\"lr\", cls2)], n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# scores = cross_validate(scls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_2, n_jobs=-1, verbose=1)\n",
    "\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "# all_f1.append(avg_fmeasure)\n",
    "# all_accs.append(avg_accuracy)\n",
    "\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.8min finished\n",
    "# print(1)\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# 2\n",
    "# cls1 = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100), random_state=RANDOM_STATE) # Classifier #1 \n",
    "# cls2 = LogisticRegression(random_state=RANDOM_STATE) # Classifier #2 \n",
    "# scls = StackingClassifier([(\"pnn\", cls1), (\"lr\", cls2)], n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# scores = cross_validate(scls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_2, n_jobs=-1, verbose=1)\n",
    "\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "# all_f1.append(avg_fmeasure)\n",
    "# all_accs.append(avg_accuracy)\n",
    "# print(2)\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# 3\n",
    "# cls1 = DecisionTreeClassifier(criterion=\"gini\", max_depth=11, random_state=RANDOM_STATE) # Classifier #1 \n",
    "# cls2 = MLPClassifier(hidden_layer_sizes=(100, 100, 100, 100), random_state=RANDOM_STATE) # Classifier #2 \n",
    "# scls = StackingClassifier([(\"tree\", cls1), (\"pnn\", cls2)], n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# scores = cross_validate(scls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_2, n_jobs=-1, verbose=1)\n",
    "\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "# all_f1.append(avg_fmeasure)\n",
    "# all_accs.append(avg_accuracy)\n",
    "# print(3)\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# 4\n",
    "cls1 = MLPClassifier(hidden_layer_sizes=(100, 100, 100), random_state=RANDOM_STATE) # Classifier #1 \n",
    "cls2 = MLPClassifier(hidden_layer_sizes=(50, 50, 50), random_state=RANDOM_STATE) # Classifier #2 \n",
    "scls = StackingClassifier([(\"pnn1\", cls1), (\"pnn2\", cls2)], n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "scores = cross_validate(scls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_2, n_jobs=-1, verbose=1)\n",
    "\n",
    "avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "# all_f1.append(avg_fmeasure)\n",
    "# all_accs.append(avg_accuracy)\n",
    "# print(4)\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "#5\n",
    "# cls1 = DecisionTreeClassifier(criterion=\"gini\", max_depth=9, random_state=RANDOM_STATE) # Classifier #1 \n",
    "# cls2 = DecisionTreeClassifier(criterion=\"gini\", max_depth=1, random_state=RANDOM_STATE) # Classifier #2 \n",
    "# scls = StackingClassifier([(\"tree1\", cls1), (\"tree2\", cls2)], n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# scores = cross_validate(scls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_2, n_jobs=-1, verbose=1)\n",
    "\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "# all_f1.append(avg_fmeasure)\n",
    "# all_accs.append(avg_accuracy)\n",
    "# print(5)\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# 6\n",
    "# cls1 = MLPClassifier(hidden_layer_sizes=(50, 50), random_state=RANDOM_STATE) # Classifier #1 \n",
    "# cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=RANDOM_STATE) # Classifier #2 \n",
    "# scls = StackingClassifier([(\"pnn1\", cls1), (\"pnn2\", cls2)], n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# scores = cross_validate(scls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_2, n_jobs=-1, verbose=1)\n",
    "\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "# all_f1.append(avg_fmeasure)\n",
    "# all_accs.append(avg_accuracy)\n",
    "# print(6)\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# 7\n",
    "# cls1 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=RANDOM_STATE) # Classifier #1 \n",
    "# cls2 = MLPClassifier(hidden_layer_sizes=(30, 30, 30), random_state=RANDOM_STATE) # Classifier #2 \n",
    "# scls = StackingClassifier([(\"pnn1\", cls1), (\"pnn2\", cls2)], n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# scores = cross_validate(scls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_2, n_jobs=-1, verbose=1)\n",
    "\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "# all_f1.append(avg_fmeasure)\n",
    "# all_accs.append(avg_accuracy)\n",
    "# print(7)\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# print(all_f1)\n",
    "# print(all_accs)\n",
    "\n",
    "#                    1                   2                 3                    4                   5\n",
    "# all_f1 = [0.8626463581917341, 0.873289137235707, 0.8707953502469138, 0.878791773845672, 0.7733577433841825, \n",
    "#   \\6,7\\        0.8734802762210417, 0.875696892380264]\n",
    "# all_accs = [0.8377082623025343, 0.8496255256279122, 0.8472479827253098, 0.8560438686214343, 0.7176042732128651, \n",
    "#   \\6,7\\    0.8496141607000796, 0.852482668485055]\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# avg_fmeasure = 0.8785732262195165 # The average f-measure of the best stacking method 4\n",
    "# avg_accuracy = 0.8551022843504944 # The average accuracy of the best stacking method 4\n",
    "\n",
    "exe_time = (time.time() - start_time)/60\n",
    "print(\"Execution time = {} min\".format(exe_time))\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a27a3122627aed7d5a6f5678055f712",
     "grade": false,
     "grade_id": "cell-6d6cadab378a2b03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "StackingClassifier(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n",
      "                   estimators=[('pnn1',\n",
      "                                MLPClassifier(hidden_layer_sizes=(100, 100,\n",
      "                                                                  100),\n",
      "                                              random_state=42)),\n",
      "                               ('pnn2',\n",
      "                                MLPClassifier(hidden_layer_sizes=(50, 50, 50),\n",
      "                                              random_state=42))],\n",
      "                   n_jobs=-1)\n",
      "F1-Score:0.878791773845672 & Accuracy:0.8560438686214343\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(scls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(avg_fmeasure,avg_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6b0c745a12e384341f4e246c5242d25",
     "grade": false,
     "grade_id": "cell-8a05446ba9a944c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.3 Report the results ###  \n",
    "Report the results of your experiments in the following cell. How did you choose your initial classifiers? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3da261e2e18ede4c057e21e080b9bac",
     "grade": true,
     "grade_id": "cell-1522ee0b7c414fba",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Απάντηση ##\n",
    "**1.3)** Στην ενότητα **1.1**, ο σκοπός ήταν να δημιουργήσουμε έναν Voting classifier με 3 estimators/classifiers και να δοκιμάσουμε τον Voting classifier με όρισμα voting=\"soft\" και voting=\"hard\". Για πρώτο classifier χρησιμοποίησα ένα Decision tree και με μια συνοπτική εξερεύνηση για διάφορες τιμές του max_depth έβαλα max_depth=6 γιατί έβγαζε το καλύτερο accuracy=0.71377. Για δεύτερο classifier χρησιμοποίησα έναν Logistic regressor ο οποίος μόνος του έδειξε αρκετά καλά αποτελέσματα με το accuracy του να φτάνει στο accuracy=0.8374582338902148. Για τρίτο και τελευταίο classifier χρησιμοποίησα ένα multi-layer Perceptron το οποίο μόνο του, έδειξε τα καλύτερα αποτελέσματα με accuracy περίπου 0.84-0.85. Στην συνέχεια, τα τρία παραπάνω μοντέλα συνδυάστηκαν με έναν VotingClassifier με τον οποίο δοκιμάστηκε και η τεχνική hard voting και η soft voting. Τελικά η καλύτερη τεχνική ήταν η soft voting με τα αποτελέσματα να είναι:\n",
    "\n",
    "* hard voting: F1-Score:0.8695284385426904 & Accuracy:0.8448556654165247\n",
    "* soft voting: F1-Score:0.8735066055502333 & Accuracy:0.849850551199\n",
    "\n",
    "Επιπλέον, ο χρόνος εκτέλεσης και στις δύο περιπτώσεις ήταν περίπου 1 λεπτό.\n",
    "\n",
    "Στη ενότητα **1.2**, ο σκοπός ήταν να δημιουργήσουμε έναν Stacking classifier με 2 estimators/classifers. Επίσης, έπρεπε να δοκιμαστεί ένας ικανοποιητικός αριθμός συνδυασμών διάφορων μοντέλων ώστε να επιλέξουμε το καλύτερο. \n",
    "\n",
    "Στην δοκιμή νούμερο **1** χρησιμοποίησα ένα Decision tree και έναν Logistic regressor και τα αποτελέσματα ήταν:\n",
    "\n",
    "* F1-Score:0.8626463581917341 & Accuracy:0.8377082623025343\n",
    "\n",
    "Στη δοκιμή νόυμερο **2** χρησιμοποίησα ένα multy-layer Perceptron με hidden_layer_sizes=(100, 100, 100, 100) και έναν Logistic regressor και τα αποτελέσματα ήταν τα εξής:\n",
    "\n",
    "* F1-Score:0.873289137235707 & Accuracy:0.8496255256279122\n",
    "\n",
    "Στη δοκιμή νόυμερο **3** επέλεξα να βάλω ένα Decision tree και πάλι ένα multy-layer Perceptron με hidden_layer_sizes=(100, 100, 100, 100) και πήρα τα παρακάτω αποτελέσματα:\n",
    "\n",
    "* F1-Score:0.8707953502469138 & Accuracy:0.8472479827253098\n",
    "\n",
    "Στη δοκιμή νόυμερο **4**, η οποία ήταν και αυτή με τα καλύτερα αποτελέσματα, χρησιμοποίησα 2 multy-layer Perceptron (μιας και φάνηκαν ότι μεμονομένα έχουν τα καλύτερα αποτελέσματα) ένα με hidden_layer_sizes=(100, 100, 100) και ένα με hidden_layer_sizes=(50, 50, 50) και πήρα τα εξής αποτελέσματα:\n",
    "\n",
    "* F1-Score:0.878791773845672 & Accuracy:0.8560438686214343\n",
    "\n",
    "Στη δοκιμή νόυμερο **5**, όπου ήταν και η δοκιμή με τα χειρότερα αποτελέσματα, χρησιμοποίησα 2 Decision trees και τα αποτελέσματα ήταν τα εξής:\n",
    "\n",
    "* F1-Score:0.7733577433841825 & Accuracy:0.7176042732128651\n",
    "\n",
    "Στη δοκιμή νόυμερο **6**, χρησιμοποίησα πάλι 2 multy-layer Perceptron αλλά αυτή τη φορά με hidden_layer_sizes=(50, 50) και hidden_layer_sizes=(25, 25, 25) και το Stacking τους έδωσε τα παρακάτω αποτελέσματα:\n",
    "\n",
    "* F1-Score:0.8734802762210417 & Accuracy:0.8496141607000796\n",
    "\n",
    "Τέλος, στη δοκιμή νόυμερο **7**, πάλι δοκιμάστηκε ένας ακόμα συνδυασμός 2 multy-layer Perceptron, αυτή τη φορά με hidden_layer_sizes=(25, 25, 25) και hidden_layer_sizes=(30, 30, 30) και τα αποτελέσματα ήταν τα παρακάτω:\n",
    "\n",
    "* F1-Score:0.875696892380264 & Accuracy:0.852482668485055\n",
    "\n",
    "Τελικά, όπως προανέφερα τα καλύτερα αποτελέσματα στο **1.2** τα έβγαλε το Stacking στη δοκιμή 4 με accuracy = 0.852725877940675.\n",
    "\n",
    "Εν γένει το Stacking φάνηκε να χρειάζεται πολύ παραπάνω execution time σε σχέση με το Voting και αυτό είναι λογικό γιατί στο Stacking πρέπει να εκπαιδευτεί και ο final_estimator. Επιπλέον, συγκριτικά voting και stacking, το voting παρόλο τον παραπάνω χρόνο που χρειάζεται φαίνεται γενικά να βγάζει καλύτερα αποτελέσματα.\n",
    "\n",
    "Να σημειωθεί ότι αρκετά από τα πειράματα που έγιναν με τη μέθοδο stacking χρειάστηκαν και 16-17 λεπτά για να τρέξουν. Γι αυτό τον λόγο τα έχω βάλει σε στυλ σχόλια ώστε να μπορέσει το notebook να τρέξει."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7520f22a6a708d14fa6d56d42b78d9f",
     "grade": false,
     "grade_id": "cell-b40c3a7c4ef32588",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.0 Randomization ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5073f6ead7355190904470c661d86d53",
     "grade": false,
     "grade_id": "cell-64c9c6881b26f5bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.1** You are asked to create three ensembles of decision trees where each one uses a different method for producing homogeneous ensembles. Compare them with a simple decision tree classifier and report your results in the dictionaries (dict) below using as key the given name of your classifier and as value the f1/accuracy score. The dictionaries should contain four different elements.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1661b594e02f8f8a73ceaad8f03b85a3",
     "grade": true,
     "grade_id": "cell-9e760b938516b506",
     "locked": false,
     "points": 30,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "Execution time = 15.186773920059204 min\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# total_scores = {}\n",
    "# for i in range(1,30):\n",
    "#     tree1 = DecisionTreeClassifier(criterion=\"gini\", max_depth=i, random_state=RANDOM_STATE)\n",
    "#     print(i)\n",
    "#     scores = cross_validate(tree1, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv, n_jobs=-1, verbose=1)\n",
    "#     total_scores[\"max depth = {}\".format(i)] = np.mean(scores[\"test_accuracy\"])\n",
    "#     print(total_scores)\n",
    "\n",
    "# 'max depth = 6': 0.7194885782475281 best score for decision tree with variable max_depth   \n",
    "# print(total_scores)\n",
    "    \n",
    "ens1 = BaggingClassifier(DecisionTreeClassifier( random_state=RANDOM_STATE), n_estimators=100, n_jobs=-1, random_state=RANDOM_STATE) # Bootstrap Aggregation\n",
    "print(\"1\")\n",
    "ens2 = RandomForestClassifier(n_estimators=100, criterion=\"gini\", bootstrap=True, n_jobs=-1, random_state=RANDOM_STATE) # Random Forest\n",
    "print(\"2\")\n",
    "ens3 = AdaBoostClassifier(DecisionTreeClassifier( random_state=RANDOM_STATE), n_estimators=100, random_state=RANDOM_STATE) # AdaBoost\n",
    "print(\"3\")\n",
    "tree = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "bagging_scores = cross_validate(ens1, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv, n_jobs=-1)\n",
    "print(\"4\")\n",
    "bagging_av_f1 = np.mean(bagging_scores[\"test_f1\"])\n",
    "bagging_av_acc = np.mean(bagging_scores[\"test_accuracy\"])\n",
    "\n",
    "rf_scores = cross_validate(ens2, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv, n_jobs=-1)\n",
    "print(\"5\")\n",
    "rf_av_f1 = np.mean(rf_scores[\"test_f1\"])\n",
    "rf_av_acc = np.mean(rf_scores[\"test_accuracy\"])\n",
    "\n",
    "adaboost_scores = cross_validate(ens3, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv, n_jobs=-1)\n",
    "print(\"6\")\n",
    "adaboost_av_f1 = np.mean(adaboost_scores[\"test_f1\"])\n",
    "adaboost_av_acc = np.mean(adaboost_scores[\"test_accuracy\"])\n",
    "\n",
    "simpletree_scores = cross_validate(tree, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv, n_jobs=-1)\n",
    "print(\"7\")\n",
    "simpletree_av_f1 = np.mean(simpletree_scores[\"test_f1\"])\n",
    "simpletree_av_acc = np.mean(simpletree_scores[\"test_accuracy\"])\n",
    "\n",
    "\n",
    "f_measures = dict()\n",
    "f_measures[\"Bagging Classifier\"] = bagging_av_f1\n",
    "f_measures[\"Random Forest Classifier\"] = rf_av_f1\n",
    "f_measures[\"AdaBoost Classifier\"] = adaboost_av_f1\n",
    "f_measures[\"Simple Tree Classifier\"] = simpletree_av_f1\n",
    "\n",
    "accuracies = dict()\n",
    "accuracies[\"Bagging Classifier\"] = bagging_av_acc\n",
    "accuracies[\"Random Forest Classifier\"] = rf_av_acc\n",
    "accuracies[\"AdaBoost Classifier\"] = adaboost_av_acc\n",
    "accuracies[\"Simple Tree Classifier\"] = simpletree_av_acc\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "# Example f_measures = {'Simple Decision':0.8551, 'Ensemble with random ...': 0.92, ...}\n",
    "######################################################################################################\n",
    "#---------------------------------------------Resutls-------------------------------------------------\n",
    "# 1\n",
    "# max_depth=6 and n_estimators=100\n",
    "# BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=6,\n",
    "#                                                         random_state=42),\n",
    "#                   n_estimators=100, n_jobs=-1, random_state=42)\n",
    "# RandomForestClassifier(max_depth=6, n_jobs=-1, random_state=42)\n",
    "# AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=6,\n",
    "#                                                          random_state=42),\n",
    "#                    n_estimators=100, random_state=42)\n",
    "# DecisionTreeClassifier(max_depth=6, random_state=42)\n",
    "# Classifier:Bagging Classifier -  F1:0.8252112379247318\n",
    "# Classifier:Random Forest Classifier -  F1:0.8173721758415123\n",
    "# Classifier:AdaBoost Classifier -  F1:0.8270621316541316\n",
    "# Classifier:Simple Tree Classifier -  F1:0.7717760239428212\n",
    "# Classifier:Bagging Classifier -  Accuracy:0.7788481645641551\n",
    "# Classifier:Random Forest Classifier -  Accuracy:0.7569104443686783\n",
    "# Classifier:AdaBoost Classifier -  Accuracy:0.7893289010114786\n",
    "# Classifier:Simple Tree Classifier -  Accuracy:0.7194885782475281\n",
    "\n",
    "# 2\n",
    "# BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "#                   n_estimators=100, n_jobs=-1, random_state=42)\n",
    "# RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "# AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "#                    n_estimators=100, random_state=42)\n",
    "# DecisionTreeClassifier(random_state=42)\n",
    "# Classifier:Bagging Classifier -  F1:0.8461894827412075\n",
    "# Classifier:Random Forest Classifier -  F1:0.8499548718685052\n",
    "# Classifier:AdaBoost Classifier -  F1:0.7395863598108929\n",
    "# Classifier:Simple Tree Classifier -  F1:0.7463826624263714\n",
    "# Classifier:Bagging Classifier -  Accuracy:0.8117178088419139\n",
    "# Classifier:Random Forest Classifier -  Accuracy:0.8129054438004317\n",
    "# Classifier:AdaBoost Classifier -  Accuracy:0.6966058643027616\n",
    "# Classifier:Simple Tree Classifier -  Accuracy:0.7037544039095353\n",
    "\n",
    "\n",
    "exe_time = (time.time() - start_time)/60\n",
    "print(\"Execution time = {} min\".format(exe_time)) # Execution time = 16.082589451471964 min\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0da6aad44e9bd8a9f7cc06eccec886c0",
     "grade": false,
     "grade_id": "cell-77f4dc2cd4cb2f7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
      "                  n_estimators=100, n_jobs=-1, random_state=42)\n",
      "RandomForestClassifier(n_jobs=-1, random_state=42)\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
      "                   n_estimators=100, random_state=42)\n",
      "DecisionTreeClassifier(random_state=42)\n",
      "Classifier:Bagging Classifier -  F1:0.8461894827412075\n",
      "Classifier:Random Forest Classifier -  F1:0.8499548718685052\n",
      "Classifier:AdaBoost Classifier -  F1:0.7395863598108929\n",
      "Classifier:Simple Tree Classifier -  F1:0.7463826624263714\n",
      "Classifier:Bagging Classifier -  Accuracy:0.8117178088419139\n",
      "Classifier:Random Forest Classifier -  Accuracy:0.8129054438004317\n",
      "Classifier:AdaBoost Classifier -  Accuracy:0.6966058643027616\n",
      "Classifier:Simple Tree Classifier -  Accuracy:0.7037544039095353\n"
     ]
    }
   ],
   "source": [
    "print(ens1)\n",
    "print(ens2)\n",
    "print(ens3)\n",
    "print(tree)\n",
    "for name,score in f_measures.items():\n",
    "    print(\"Classifier:{} -  F1:{}\".format(name,score))\n",
    "for name,score in accuracies.items():\n",
    "    print(\"Classifier:{} -  Accuracy:{}\".format(name,score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "48f3eafac80f6dd15441c4991c78836d",
     "grade": false,
     "grade_id": "cell-a6ea07f0be814a40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.2** Describe your classifiers and your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9395efc3b936166e55b3bec6e0afdab3",
     "grade": true,
     "grade_id": "cell-399fc5e7254f1c58",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Απάντηση ##\n",
    "**2.2)** Σε αυτή την ενότητα ο σκοπός είναι να φτιάξουμε και να δοκιμάσουμε 3 ομογενή ensembles τα οποία θα απαρτίζονται απολειστικά και μόνο από Decision trees ώστε να τα συγκρίνουμε με ένα απλό Decision tree. Τα ομογενή ensembles που χρησιμοποίησα είναι με σειρά τα εξής:\n",
    "\n",
    "1. Bagging Classifier με 100 estimators\n",
    "2. Random Forest με 100 estimators\n",
    "3. AdaBoost με 100 estimators\n",
    "\n",
    "Έγιναν 2 ομάδες πειραμάτων. Στην πρώτη ομάδα ο base classifier για όλα τα ensembles αλλά και για το μεμονομένο Decision tree ήταν ο απλός classifier DecisionTreeClassifier( random_state=RANDOM_STATE) και στη δεύτερη ομάδα πειραμάτων ο base classifier για όλα τα ensembles και το μεμονομένο Decision tree ήταν ο DecisionTreeClassifier(max_depth=6, random_state=42). Έβαλα max_depth=6 διότι με μιά διερεύνηση στην αρχή του κώδικα φάνηκε ότι το Decision tree με το καλύτερο accuracy και με μεταβλητό μέγεθος μόνο το max_depth είναι εκείνο το Decision tree με max_depth=6.\n",
    "\n",
    "Τα αποτελέσματα των πειραμάτων φαίνονται στον παρακάτω πίνακα:\n",
    "\n",
    "* base_estimator=DecisionTreeClassifier( random_state=RANDOM_STATE)\n",
    "\n",
    "|               | Bagging            | Random Forest      | AdaBoost           | Decision Tree      |\n",
    "| ------------- |:------------------:| ------------------:|-------------------:|-------------------:|\n",
    "| **F1_score**  | 0.8461894827412075 | 0.8499548718685052 | 0.7395863598108929 | 0.7463826624263714 |\n",
    "| **Accuracy**  | 0.8117178088419139 | 0.8129054438004317 | 0.6966058643027616 | 0.7037544039095353 |\n",
    "\n",
    "* base_estimator=DecisionTreeClassifier(max_depth=6, random_state=RANDOM_STATE)\n",
    "\n",
    "|               | Bagging            | Random Forest      | AdaBoost           | Decision Tree      |\n",
    "| ------------- |:------------------:| ------------------:|-------------------:|-------------------:|\n",
    "| **F1_score**  | 0.8252112379247318 | 0.8173721758415123 | 0.8270621316541316 | 0.7717760239428212 |\n",
    "| **Accuracy**  | 0.7788481645641551 | 0.7569104443686783 | 0.7893289010114786 | 0.7194885782475281 |\n",
    "\n",
    "\n",
    "Από τους παραπάνω πίνακες, φαίνεται ότι στη πρώτη ομάδα πειραμάτων χωρίς max_depth, τα μοντέλα του Bagging και του Random Forest ευνοήθηκαν από τον base estimator που δεν είχε max_depth και έτσι ο κάθε estimator από τους συνολικά 100 είχε την δυνατότητα να κάνει υπερκπαίδευση. Από την άλλη πλευρά τα μοντέλα AdaBoost και Decision Tree(προφανώς) φαίνεται να επηρεάστηκαν αρνητικά από την απουσία ορίου στο max_depth.\n",
    "\n",
    "Τελικά, και στις 2 ομάδες πειραμάτων τα ensembles Bagging και Random Forest είχαν καλύτερα αποτελέσματα από το απλό Decision Tree. Το ensemble τύπου AdaBoost οταν δεν υπήρχε όριο στο max_depth, έδωσε χειρότερα αποτελέσματα από το αντίστοιχο Decision Tree, ενώ στο πείραμα όπου υπήρχε όριο στο max_depth ο AdaBoost έδωσε καλύτερα αποτελέσματα από το Decision Tree.\n",
    "\n",
    "Να σημειωθεί και εδώ ότι στο δικό μου πσ αυτή η ενότητα έκανε να τρέξει περίπου 16.08 λεπτά με 4 πυρήνες.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8edad6471eab5b0645f6ca46bab2f7a",
     "grade": false,
     "grade_id": "cell-a0de461bc76e0880",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**2.3** Increasing the number of estimators in a bagging classifier can drastically increase the training time of a classifier. Is there any solution to this problem? Can the same solution be applied to boosting classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9573961dbe26d9ce6669df5da70a45a",
     "grade": true,
     "grade_id": "cell-0a28025407c78a48",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Απάντηση ##\n",
    "**2.3)** Για τον Bagging classifier υπάρχει λύση και αυτή είναι να τρέξουν τα επιμέρους μοντέλα παράλληλα, μιας και το καθένα δεν εξαρτάται απο κάποιο άλλο. Αντίθετα, για τον boosting classifier δεν υπάρχει λύση γιατί οι boosting αλγόριθμοι είναι φτιαγμένοι για να τρέχουν σειριακά μιας και ο κάθε επόμενος classifier του ensemble πρέπει να κοιτάει τα λάθη του προηγούμενου, ώστε να δίνει εκεί βαρύτητα στην εκπαίδευση του."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d016d23320010c4d8d9e8218cba553e8",
     "grade": false,
     "grade_id": "cell-35e46873d8c6537c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3.0 Creating the best classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7e2a127a27e4c9131e94ae73e6e325b",
     "grade": false,
     "grade_id": "cell-6de6582e696ba2d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.1** In this part of the assignment you are asked to train the best possible ensemble! Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code. Can you achieve an accuracy over 83-84%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8d58b62ea96a7e0e0776f79c58e4b10",
     "grade": true,
     "grade_id": "cell-d1bba508731c9030",
     "locked": false,
     "points": 20,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 0.0 min\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "start_time = time.time()\n",
    "\n",
    "# LogisticRegression(random_state=42)\n",
    "# F1-Score:0.862477756053269 & Accuracy:0.8374582338902148\n",
    "\n",
    "# MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "# F1-Score:0.8718641047481654 & Accuracy:0.8479486305261961\n",
    "\n",
    "# MLPClassifier(hidden_layer_sizes=(25, 25), random_state=42)\n",
    "# F1-Score:0.8713195550282344 & Accuracy:0.8484231162632117\n",
    "\n",
    "# MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "# F1-Score:0.8729263197568979 & Accuracy:0.8500875099443117\n",
    "\n",
    "# RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "# F1-Score:0.8509367656799831 & Accuracy:0.8148192976474599\n",
    "\n",
    "# LogisticRegression(max_iter=300, random_state=42)\n",
    "# F1-Score:0.8565742960319722 & Accuracy:0.830310262529833\n",
    "\n",
    "# BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=42),\n",
    "#                   max_features=0.5, max_samples=3000, n_estimators=20,\n",
    "#                   n_jobs=-1, random_state=42)\n",
    "# F1-Score:0.8368044407262497 & Accuracy:0.8041141038754404\n",
    "\n",
    "# RandomForestClassifier(max_features=128, max_samples=1500, n_jobs=-1,\n",
    "#                        random_state=42)\n",
    "# F1-Score:0.8432139121826605 & Accuracy:0.8045914308444141\n",
    "        \n",
    "        \n",
    "# cv_1 = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "# cv_2 = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# cls1 = LogisticRegression(random_state=42)\n",
    "# cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "# cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "# cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "# cls5 = BaggingClassifier(DecisionTreeClassifier( random_state=RANDOM_STATE), n_estimators=50, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "# cls6 = LogisticRegression(random_state=42, max_iter=300)\n",
    "# cls7 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=500)\n",
    "# cls8 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1000)\n",
    "# cls9 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=2000)\n",
    "# cls10 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=3000)\n",
    "# cls11 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=3500)\n",
    "\n",
    "# cls12 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=500, max_features=64)\n",
    "# cls13 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=500, max_features=128)\n",
    "# cls14 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1000, max_features=64)\n",
    "# cls15 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1000, max_features=128)\n",
    "# cls16 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=2000, max_features=64)\n",
    "# cls17 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=2000, max_features=128)\n",
    "# cls18 = RandomForestClassifier(n_jobs=-1, random_state=42, max_features=128)\n",
    "# cls19 = RandomForestClassifier(n_jobs=-1, random_state=42, max_features=256)\n",
    "# cls20 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1500, max_features=64)\n",
    "# cls21 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1500, max_features=128)\n",
    "# cls22 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=250, max_features=1000)\n",
    "\n",
    "# cls23 = BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), \n",
    "#                           max_features=0.5, max_samples=3000, n_estimators=20, n_jobs=-1, random_state=42)\n",
    "# cls24 = MLPClassifier(hidden_layer_sizes=(25, 25), random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# comb_cls = [(\"lr\", cls1), (\"pnn1\", cls2), (\"pnn2\", cls3), (\"rf\", cls4)]\n",
    "# comb_cls2 = [(\"lr\", cls1), (\"pnn1\", cls2), (\"pnn2\", cls3), (\"rf\", cls4), (\"bagc\", cls5)]\n",
    "# comb_cls3 = [(\"lr\", cls1), (\"pnn1\", cls2), (\"pnn2\", cls3), (\"rf1\", cls4), (\"rf2\", cls5), \n",
    "#              (\"rf3\", cls6), (\"rf4\", cls7), (\"rf5\", cls8), (\"rf6\", cls9), (\"rf7\", cls10)]\n",
    "# comb_cls4_only_rf = [(\"rf1\", cls12), (\"rf2\", cls13), (\"rf3\", cls14), (\"rf4\", cls15), (\"rf5\", cls16), \n",
    "#              (\"rf6\", cls17), (\"rf7\", cls18), (\"rf8\", cls19), (\"rf9\", cls20), (\"rf10\", cls21), (\"rf11\", cls22)]\n",
    "# comb_cls5 = [(\"lr1\", cls1), (\"lr2\", cls6), (\"pnn1\", cls2), (\"pnn2\", cls3), (\"rf1\", cls4), \n",
    "#              (\"rf2\", cls21), (\"baggin1\", cls23)]\n",
    "# comb_cls6 = [(\"lr1\", cls1), (\"lr2\", cls6), (\"pnn1\", cls2), (\"pnn2\", cls3), (\"rf1\", cls4), \n",
    "#              (\"rf2\", cls21), (\"baggin1\", cls23), (\"pnn3\", cls24)]\n",
    "# comb_cls7 = [(\"lr1\", cls1), (\"lr2\", cls6), (\"pnn1\", cls2), (\"pnn2\", cls3), (\"pnn3\", cls24)]\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 1\n",
    "# scls = StackingClassifier(comb_cls, n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# scores = cross_validate(scls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 18.2min finished\n",
    "# F1: 0.8797233588670561\n",
    "# Accuracy: 0.8570150017047393\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 2\n",
    "# vcls = VotingClassifier(comb_cls, voting=\"soft\", n_jobs=-1)\n",
    "\n",
    "# scores = cross_validate(vcls, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.8min finished\n",
    "# F1: 0.8738573463957859\n",
    "# Accuracy: 0.8501011478577112\n",
    "#-------------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 3\n",
    "# scls2 = StackingClassifier(comb_cls2, n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# scores = cross_validate(scls2, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# Too long cv duration, pc didnt manage to complete the task\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 4\n",
    "# vcls2 = VotingClassifier(comb_cls2, voting=\"soft\", n_jobs=-1)\n",
    "\n",
    "# scores = cross_validate(vcls2, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  9.7min finished\n",
    "# F1: 0.8758596207980501\n",
    "# Accuracy: 0.85200136379134\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 5\n",
    "# vcls3 = VotingClassifier(comb_cls3, voting=\"hard\", n_jobs=-1)\n",
    "\n",
    "# scores = cross_validate(vcls3, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 10.4min finished\n",
    "# F1: 0.8636663362594531\n",
    "# Accuracy: 0.8338822593476533\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 6\n",
    "# vcls4 = VotingClassifier(comb_cls4_only_rf, voting=\"soft\", n_jobs=-1)\n",
    "\n",
    "# scores = cross_validate(vcls4, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.4min finished\n",
    "# F1: 0.8399286296040069\n",
    "# Accuracy: 0.797897488350949\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 7\n",
    "# vcls5 = VotingClassifier(comb_cls5, voting=\"hard\", n_jobs=-1)\n",
    "\n",
    "# scores = cross_validate(vcls5, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.6min finished\n",
    "# F1: 0.875042022581448\n",
    "# Accuracy: 0.8500931924082282\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 8\n",
    "# vcls6 = VotingClassifier(comb_cls5, voting=\"soft\", n_jobs=-1)\n",
    "\n",
    "# scores = cross_validate(vcls6, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  3.6min finished\n",
    "# F1: 0.8757143523211848\n",
    "# Accuracy: 0.8520042050232981\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 9\n",
    "# scls3 = StackingClassifier(comb_cls5, n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# scores = cross_validate(scls3, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_2, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed: 36.2min finished\n",
    "# F1: 0.8766826028695803\n",
    "# Accuracy: 0.8529509035117627\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 10\n",
    "# vcls7 = VotingClassifier(comb_cls6, voting=\"soft\", n_jobs=-1)\n",
    "\n",
    "# scores = cross_validate(vcls7, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.2min finished\n",
    "# F1: 0.8773199406697685\n",
    "# Accuracy: 0.8539078304352768\n",
    "# -------------------------------------------------------------------------------------------------------------------------\n",
    "# Peirama 11\n",
    "# vcls8 = VotingClassifier(comb_cls7, voting=\"soft\", n_jobs=-1)\n",
    "\n",
    "# scores = cross_validate(vcls8, X, y, scoring=[\"f1\", \"accuracy\"], cv=cv_1, n_jobs=-1, verbose=1)\n",
    "# avg_fmeasure = np.mean(scores[\"test_f1\"]) # The average f-measure\n",
    "# avg_accuracy = np.mean(scores[\"test_accuracy\"]) # The average accuracy\n",
    "\n",
    "# print(avg_fmeasure)\n",
    "# print(avg_accuracy)\n",
    "# results of this enseble\n",
    "# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
    "# [Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.9min finished\n",
    "# F1: 0.8744252999938474\n",
    "# Accuracy: 0.8512887828162292\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Best model number 1\n",
    "cv_1 = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cls1 = LogisticRegression(random_state=42)\n",
    "cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "comb_cls = [(\"lr\", cls1), (\"pnn1\", cls2), (\"pnn2\", cls3), (\"rf\", cls4)]\n",
    "\n",
    "best_cls = StackingClassifier(comb_cls, n_jobs=-1, cv=cv_1, passthrough=False ) # Stacking Classifier\n",
    "\n",
    "# elapsed: 18.2min finished\n",
    "best_fmeasure = 0.8797233588670561\n",
    "best_accuracy = 0.8570150017047393\n",
    "\n",
    "exe_time = (time.time() - start_time)/60\n",
    "print(\"Execution time = {} min\".format(exe_time))\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ecc691ce956fad47c497b9849747c34",
     "grade": false,
     "grade_id": "cell-39673f451b660dcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier:\n",
      "StackingClassifier(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n",
      "                   estimators=[('lr', LogisticRegression(random_state=42)),\n",
      "                               ('pnn1',\n",
      "                                MLPClassifier(hidden_layer_sizes=(25, 25, 25),\n",
      "                                              random_state=42)),\n",
      "                               ('pnn2',\n",
      "                                MLPClassifier(hidden_layer_sizes=(15, 15),\n",
      "                                              random_state=42)),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(n_jobs=-1,\n",
      "                                                       random_state=42))],\n",
      "                   n_jobs=-1)\n",
      "F1-Score:0.8797233588670561 & Accuracy:0.8570150017047393\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier:\")\n",
    "print(best_cls)\n",
    "print(\"F1-Score:{} & Accuracy:{}\".format(best_fmeasure,best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "667632db5afb6f143062507bae31063f",
     "grade": false,
     "grade_id": "cell-6a072817c64ce4a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.2** Describe the process you followed to achieve this result. How did you choose your classifier and your parameters and why. Report the f-measure & accuracy (10-fold cross validation) of your final classifier and results of classifiers you tried in the cell following the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3abcb52a70fcb6da4f93980026b8e593",
     "grade": true,
     "grade_id": "cell-5f1d5ba45ffeb074",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "## Απάντηση ##\n",
    "**3.2)** Από την **1.1** είδαμε ότι ο Logistic Regressor αλλά και το multy-layer Perceptron βγάζαν τα καλύτερα αποτελέσματα όταν χρησιμοποιούνταν μεμονομένα. Στην συνέχεια στην **2.2** είδα ότι υπάρχουν και ομογενή ensembles τα οποία επίσης βγάζουν αρκετά καλά accuracies, όπως είναι ο Bagging Classifier και το Random Forest. Σε αυτή την ενότητα συνεπώς, γίνανε 11 πειράματα με συνδυασμούς τον προαναφερόμενων μοντέλων/classifiers ώστε να βρεθεί το καλύτερο ensemble. Επίσης, στα αρχικά σχόλια του κώδικα στην ενότητα **3.1** φαίνονται οι αποδόσεις κάποιων μεμονομένων μοντέλων που μας προέτρεψαν να τα χρησιμοποιήσουμε λόγω της καλής τους ατομικής απόδοσης. \n",
    "\n",
    "### Πείραμα 1 ###\n",
    "StackingClassifier με :\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "### Πείραμα 2 ###\n",
    "VotingClassifier(voting=\"soft\") με:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "### Πείραμα 3 ###\n",
    "StackingClassifier με:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "* cls5 = BaggingClassifier(DecisionTreeClassifier( random_state=RANDOM_STATE), n_estimators=50, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "### Πείραμα 4 ###\n",
    "VotingClassifier(voting=\"soft\") με:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "* cls5 = BaggingClassifier(DecisionTreeClassifier( random_state=RANDOM_STATE), n_estimators=50, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "### Πείραμα 5 ###\n",
    "VotingClassifier(voting=\"hard\") με:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "* cls5 = BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), n_estimators=50, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "* cls6 = LogisticRegression(random_state=42, max_iter=300)\n",
    "* cls7 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=500)\n",
    "* cls8 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1000)\n",
    "* cls9 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=2000)\n",
    "* cls10 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=3000)\n",
    "\n",
    "### Πείραμα 6 ###\n",
    "VotingClassifier(voting=\"soft\") με σκέτο Random Forest Classifiers:\n",
    "* cls12 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=500, max_features=64)\n",
    "* cls13 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=500, max_features=128)\n",
    "* cls14 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1000, max_features=64)\n",
    "* cls15 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1000, max_features=128)\n",
    "* cls16 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=2000, max_features=64)\n",
    "* cls17 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=2000, max_features=128)\n",
    "* cls18 = RandomForestClassifier(n_jobs=-1, random_state=42, max_features=128)\n",
    "* cls19 = RandomForestClassifier(n_jobs=-1, random_state=42, max_features=256)\n",
    "* cls20 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1500, max_features=64)\n",
    "* cls21 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1500, max_features=128)\n",
    "* cls22 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=250, max_features=1000)\n",
    "\n",
    "### Πείραμα 7 ###\n",
    "VotingClassifier(voting=\"hard\") με:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls6 = LogisticRegression(random_state=42, max_iter=300)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "* cls21 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1500, max_features=128)\n",
    "* cls23 = BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), max_features=0.5, max_samples=3000, n_estimators=20, n_jobs=-1, random_state=42)\n",
    "\n",
    "\n",
    "### Πείραμα 8 ###\n",
    "VotingClassifier(voting=\"soft\") με:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls6 = LogisticRegression(random_state=42, max_iter=300)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "* cls21 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1500, max_features=128)\n",
    "* cls23 = BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), max_features=0.5, max_samples=3000, n_estimators=20, n_jobs=-1, random_state=42)\n",
    "\n",
    "### Πείραμα 9 ###\n",
    "StackingClassifier με:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls6 = LogisticRegression(random_state=42, max_iter=300)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "* cls21 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1500, max_features=128)\n",
    "* cls23 = BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), max_features=0.5, max_samples=3000, n_estimators=20, n_jobs=-1, random_state=42)\n",
    "\n",
    "### Πείραμα 10 ###\n",
    "VotingClassifier(voting=\"soft\") με:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls6 = LogisticRegression(random_state=42, max_iter=300)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "* cls21 = RandomForestClassifier(n_jobs=-1, random_state=42, max_samples=1500, max_features=128)\n",
    "* cls23 = BaggingClassifier(DecisionTreeClassifier(random_state=RANDOM_STATE), max_features=0.5, max_samples=3000, n_estimators=20, n_jobs=-1, random_state=42)\n",
    "* cls24 = MLPClassifier(hidden_layer_sizes=(25, 25), random_state=42)\n",
    "\n",
    "### Πείραμα 11 ###\n",
    "VotingClassifier(voting=\"soft\") με:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls6 = LogisticRegression(random_state=42, max_iter=300)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls24 = MLPClassifier(hidden_layer_sizes=(25, 25), random_state=42)\n",
    "\n",
    "## Αποτελέσματα πειραμάτων σε πίνακα: ##\n",
    "\n",
    "|                | F1_score           | Accuracy           | 10_KFold_CV_Duration  | \n",
    "| ---------------|:------------------:|:------------------:|:---------------------:|\n",
    "| **Πείραμα 1**  | 0.8797233588670561 | 0.8570150017047393 | 18.2min               | \n",
    "| **Πείραμα 2**  | 0.8738573463957859 | 0.8501011478577112 | 1.8min                | \n",
    "| **Πείραμα 3**  | -                  | -                  | pc didn't manage to complete the task  | \n",
    "| **Πείραμα 4**  | 0.8758596207980501 | 0.85200136379134   | 9.7min                | \n",
    "| **Πείραμα 5**  | 0.8636663362594531 | 0.8338822593476533 | 10.4min               | \n",
    "| **Πείραμα 6**  | 0.8399286296040069 | 0.797897488350949  | 2.4min                | \n",
    "| **Πείραμα 7**  | 0.875042022581448  | 0.8500931924082282 | 3.6min                | \n",
    "| **Πείραμα 8**  | 0.8757143523211848 | 0.8520042050232981 | 3.6min                | \n",
    "| **Πείραμα 9**  | 0.8766826028695803 | 0.8529509035117627 | 36.2min               | \n",
    "| **Πείραμα 10** | 0.8773199406697685 | 0.8539078304352768 | 4.2min                | \n",
    "| **Πείραμα 11** | 0.8744252999938474 | 0.8512887828162292 | 2.9min                | \n",
    "\n",
    "### Συμπεράσματα  ###\n",
    "Όπως φαίνεται ξεκάθαρα από τον παραπάνω πίνακα το καλύτερο ensemble που κατάφερα να παράξω ήταν το ensemble στο **Πείραμα 1** το οποίο αποτελείται από τα εξής μοντέλα:\n",
    "* cls1 = LogisticRegression(random_state=42)\n",
    "* cls2 = MLPClassifier(hidden_layer_sizes=(25, 25, 25), random_state=42)\n",
    "* cls3 = MLPClassifier(hidden_layer_sizes=(15, 15), random_state=42)\n",
    "* cls4 = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "και χρησιμοποιεί την μέθοδο Stacking.\n",
    "\n",
    "Τα F1_score και Accuracy αντίστοιχα είναι: 0.8797233588670561 και 0.8570150017047393 το οποία είναι αρκετά πάνω από το threshold που ορίστηκε στα 0.83-0.84 Accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22500bda285c8dc9375ee048e883be55",
     "grade": false,
     "grade_id": "cell-5b27d068d1fbfa37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**3.3** Create a classifier that is going to be used in production - in a live system. Use the *test_set_noclass.csv* to make predictions. Store the predictions in a list.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c40bf6a2d6e630b217742246c20d2560",
     "grade": true,
     "grade_id": "cell-ab69a2863e87fd72",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time = 2.313756473859151 min\n"
     ]
    }
   ],
   "source": [
    "# BEGIN CODE HERE\n",
    "start_time = time.time()\n",
    "\n",
    "cls = best_cls\n",
    "test_set = pd.read_csv(\"test_set_noclass.csv\")\n",
    "best_cls.fit(X,y)\n",
    "predictions = best_cls.predict(test_set)\n",
    "\n",
    "exe_time = (time.time() - start_time)/60\n",
    "print(\"Execution time = {} min\".format(exe_time))\n",
    "# training lasted for 2.3841155846913655 min\n",
    "#END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ace9dbe06e5607ddf9353befef8472c0",
     "grade": false,
     "grade_id": "cell-d98d6687c3bbe4ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingClassifier(cv=StratifiedKFold(n_splits=10, random_state=42, shuffle=True),\n",
      "                   estimators=[('lr', LogisticRegression(random_state=42)),\n",
      "                               ('pnn1',\n",
      "                                MLPClassifier(hidden_layer_sizes=(25, 25, 25),\n",
      "                                              random_state=42)),\n",
      "                               ('pnn2',\n",
      "                                MLPClassifier(hidden_layer_sizes=(15, 15),\n",
      "                                              random_state=42)),\n",
      "                               ('rf',\n",
      "                                RandomForestClassifier(n_jobs=-1,\n",
      "                                                       random_state=42))],\n",
      "                   n_jobs=-1)\n",
      "[1 0 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(cls)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1af1d441fd486d53e45173d4f352ba8c",
     "grade": false,
     "grade_id": "cell-966633c679d5c960",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "LEAVE HERE ANY COMMENTS ABOUT YOUR CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments #\n",
    "**3.3**) Το ensemble που χρησιμοποιήθηκε είναι αυτό του πειράματος 1. Ο χρόνος training στο train_set που μας δόθηκε είναι στα 2.38 λεπτά.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24dbe2151df25b6e5b36e988a8e38dcb",
     "grade": false,
     "grade_id": "cell-78ffc0c68225fb1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### This following cell will not be executed. The test_set.csv with the classes will be made available after the deadline and this cell is for testing purposes!!! Do not modify it! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddcf51aaeaaa305540873fd0012a4b06",
     "grade": false,
     "grade_id": "cell-7946d9ee342bf549",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_set.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-56c0708e6408>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfinal_test_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_set.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mground_truth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_test_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CLASS'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F1-Score:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mground_truth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_set.csv'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "final_test_set = pd.read_csv('test_set.csv')\n",
    "ground_truth = final_test_set['CLASS']\n",
    "print(\"Accuracy:{}\".format(accuracy_score(predictions,ground_truth)))\n",
    "print(\"F1-Score:{}\".format(f1_score(predictions,ground_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
